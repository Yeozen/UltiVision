{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#install these dependencies, only\n",
        "!python -m pip install mediapipe\n",
        "!pip install tensorflow scikit-learn\n",
        "!wget https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker_lite/float16/latest/pose_landmarker_lite.task"
      ],
      "metadata": {
        "id": "2-NnFJb1mq7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "import joblib\n",
        "\n",
        "# Load the trained model, its in the zip file, the model is named frisbee.keras, the transformer and no-depth coord variants are also there,\n",
        "# named transformer_frisbee.keras and frisbee_no_z.keras respectively\n",
        "model = load_model('/path/to/the/model/frisbee.keras')\n",
        "\n",
        "# Load the LabelEncoder, also found in the zip file, its also named as label_encoder.joblib\n",
        "label_encoder = joblib.load('/path/to/the/label_encoder.joblib')\n",
        "\n",
        "# Define the indices of the desired landmarks (11-16 and 23-32)\n",
        "desired_landmark_indices = list(range(11, 17)) + list(range(23, 33))\n",
        "\n",
        "def extract_keypoints(video_path):\n",
        "    # Initialize MediaPipe Pose for each video\n",
        "    mp_pose = mp.solutions.pose.Pose()\n",
        "\n",
        "    # Open video capture\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    # List to store selected landmarks\n",
        "    selected_landmarks = []\n",
        "\n",
        "    while True:\n",
        "        # Read a frame\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Convert the frame to RGB\n",
        "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Process the frame with MediaPipe Pose\n",
        "        results = mp_pose.process(rgb_frame)\n",
        "\n",
        "        # Extract and store selected landmarks if available\n",
        "        if results.pose_landmarks:\n",
        "            # Create a new list for each frame to store the landmarks\n",
        "            landmarks_frame = []\n",
        "\n",
        "            for idx in desired_landmark_indices:\n",
        "                landmark = results.pose_landmarks.landmark[idx]\n",
        "\n",
        "                landmarks_frame.append({\n",
        "                    \"index\": idx,\n",
        "                    \"name\": mp.solutions.pose.PoseLandmark(idx).name,\n",
        "                    \"x\": landmark.x,\n",
        "                    \"y\": landmark.y,\n",
        "                    \"z\": landmark.z\n",
        "                })\n",
        "\n",
        "            # Append the landmarks of the current frame to the main list\n",
        "            selected_landmarks.append(landmarks_frame)\n",
        "\n",
        "    # Release the video capture\n",
        "    cap.release()\n",
        "\n",
        "    # Close the MediaPipe Pose instance\n",
        "    mp_pose.close()\n",
        "\n",
        "    return selected_landmarks\n",
        "\n",
        "max_sequence_length = 60\n",
        "\n",
        "# Path to the video or directory\n",
        "path = '/path/to/your/uploaded_video.mp4'\n",
        "\n",
        "# Check if the path is a directory or a file\n",
        "if os.path.isdir(path):\n",
        "    video_files = [os.path.join(path, f) for f in os.listdir(path) if f.endswith('.mp4')]\n",
        "elif os.path.isfile(path) and path.endswith('.mp4'):\n",
        "    video_files = [path]\n",
        "else:\n",
        "    raise ValueError(\"The path is not a valid directory or video file\")\n",
        "\n",
        "# Process each video file\n",
        "for video_path in video_files:\n",
        "    # Extract keypoints from the video\n",
        "    keypoints = extract_keypoints(video_path)\n",
        "\n",
        "    # Convert keypoints to the required format\n",
        "    sequence_data = [[[entry['x'], entry['y'], entry['z']] for entry in frame] for frame in keypoints]\n",
        "\n",
        "    # Pad the extracted keypoints to match the shape of the training data\n",
        "    keypoints_padded = pad_sequences([sequence_data], maxlen=max_sequence_length, padding='post', dtype='float32')\n",
        "\n",
        "    # Use the model to predict the class of the video\n",
        "    prediction = model.predict(keypoints_padded)\n",
        "    predicted_class_index = np.argmax(prediction, axis=-1).flatten()\n",
        "    predicted_class = label_encoder.inverse_transform(predicted_class_index)\n",
        "\n",
        "    # Get the confidence score of the predicted class\n",
        "    confidence_score = prediction[0][predicted_class_index[0]]\n",
        "\n",
        "    print(f'Predicted class for {os.path.basename(video_path)}: {predicted_class[0]}')\n",
        "    print(f'Confidence score: {confidence_score}')"
      ],
      "metadata": {
        "id": "Durc3atRmwO_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}